---
title: "Practical Machine Learning - HW 1"
author: "Casey C"
date: "Sunday, August 24, 2014"
output: html_document
---

Read in dataset
---------------
```{r}
#Load the caret package and other relevant packages
library(caret)
library(lsr)
library(kernlab)
library(rattle)
library(evaluate)
library(formatR)

#Set working directory so I can reference the files I need:
setwd("C:/Users/ccorc244/Documents/04 - Classes/2014-08 - Machine Learning/Homework 1")


#Read in the CSV file
training_all <- read.csv(file="pml-training.csv",head=TRUE, sep=",")

```

Cross validation with the initial dataset
-----------------------------------------
The initial dataset consisted of 19,622 observations.  To employ cross validation, I have gone with a 75%/25% split, allocating 75% of the training dataset provided to be my training set and then remaining 25% to be my testing dataset.  The training set contained 14,718 observations and the testing dataset contained 4,904.  I will employ several machine learning algorithms to determine the best predictor for the outcome variable and then run this algorithm on the test dataset that contains 25% of my original dataset.  I will then apply this same algorithm to the assigned 20 data points not included in the training.csv file.
```{r}
#Create a training set and a testing set
inTrain <- createDataPartition(y=training_all$classe, p=0.75, list=FALSE)
training_new <- training_all[inTrain,]
testing_new <- training_all[-inTrain,]
```

Cleaning the dataset
--------------------
After examining this dataset, I started off by removing variables associated with skew, kurtosis, average, variance, etc.  These columns were associated with rows that consisted of multiple measurements that were consolidated into one row.  As the dataset was quite limited with respect to observations that contained these features, I opted to remove them from the model.  This left 56 variables for analysis out of the original 160 variables.
```{r}
#Subset out columns related to kurtosis, skewness, min, max, stddev, avg, amplitude
#Also remove anything that is a factor or text, don't want to have to make dummy variables for these things
training_new <- training_new[,-grep("kurtosis|skewness|max_|min_|var_|stddev_|avg_|amplitude_|X|user_name|cvtd_timestamp|new_window", colnames(training_all))]
```

I checked to see if I should remove additional variables (specifically non-zero values) by applying the nearZeroVar function.  The results of this function didn't suggest any additional variables that should be removed beyond what was initially removed from the first pass.
```{r}
#Checking for near zero values - Didnt see anything crazy here
nsv <- nearZeroVar(training_new, saveMetrics=TRUE)
nsv
```

Lastly, I looked for highly correlated variables in order to examine potential principal components for variable reduction.  After examining the results of this method, I opted to forgo the principal component route in order to more easily interpret the results of whatever model I settled on.
```{r}
#Check correlation matrix
nums<-sapply(training_new, is.numeric)
training_num_only <- (training_new[,nums])
M<-abs(cor(training_num_only))
diag(M) <- 0
which(M > 0.8, arr.ind=T)


#Create some principal components - Didn't really result in much
preProc <- preProcess(training_new[,-56], method="pca", pcaComp=2)
trainPC <- predict(preProc, training_new[,-56])
plot(trainPC[, 1], trainPC[, 2], col=training_new[,56])
```

Applying machine learning algorithms
------------------------------------
**Decision Tree**
I initially ran a decision tree analysis; however, the resulting accuracy estimate of 0.53 was not promising.
```{r}
#Decision tree
modFitDT <- train(classe ~ ., method="rpart", data=training_new)
print(modFitDT)
confusionMatrix(predict(modFitDT, newdata=training_new), training_new$classe)
#fancyRpartPlot(modFitDT$finalModel)
tableDT <- table(predict(modFitDT, newdata=training_new),training_new$classe)
ftable(tableDT)
```

**Random Forest**
Unfortunately this algorithm was too strenuous for my computer given the default parameters.  I could have tried playing around with the options to try to get this method to work, but I decided to exhaust the other options first.
```{r}
#Random forest - Too strenuous for computer :( :( 
#modFitRF <- train(classe ~ ., method="rf", data=training_new, prox=TRUE)
#modFitRF
``` 

**Linear Discriminant Analysis**
The accuracy using this method was only slightly better than the decision tree analysis.  I was able to achieve an accuracy of 0.71 with this method.
```{r}
#Try linear discriminant analysis - 71% accuracy, meh
modFitLDA = train(classe ~ ., data=training_new, method="lda")
print(modFitLDA)
confusionMatrix(predict(modFitLDA, newdata=training_new), training_new$classe)
table(predict(modFitLDA, newdata=training_new),training_new$classe)
```

**Boosting with Decision Trees**
This algorithm resulted in the best classification results as it takes repeated applications of weaker decision tree predictions to create a stronger prediction.  Employing this method led to an in-sample accuracy estimate of >0.99.  As this method was promising with regards to the in-sample data, I was ready to apply the model predictors to the test dataset I had created at the beginning of the analysis.
```{r}
#Boosting w/decision trees - Pretty good success!
modFitBO <- train(classe ~ ., method="gbm", data=training_new, verbose=FALSE)
options(digits=10)
print(modFitBO)
```

I also took some time to understand what the largest drivers were for this analysis.  The largest driver of classification here is the timestamp variable.  This makes sense as the form classification of weight lifting (outcome variable) may degrade as a participant continuously lifts weights.  Another variable which had a high influence on the classification was the roll belt measurement, which suggests that changes in this measurement impact the form of participants.
```{r}
summary(modFitBO)
table(predict(modFitBO, newdata=training_new),training_new$classe)


#Calculate in sample error (though we get this from the output, I just wanted additional significant digits)
check_in_sample <- predict(modFitBO, newdata=training_new) == training_new$classe
in_sample_accuracy <- table(check_in_sample)["TRUE"]/length(check_in_sample)
in_sample_accuracy
```

Estimating out of sample error
------------------------------
To understand the out of sample error, we'd want to look at the accuracy estimates on our test dataset (out of sample error) and compare it to the accuracy estimate on our training dataset (in-sample error).  Weâ€™d expect the out of sample error to be larger than the in-sample error.  To calculate the out of sample accuracy and kappa levels, I ran a confusion matrix on the predicted classifications based on my boosting decision tree model with the actual classifications from the testing dataset I had created.

|          | In Sample Error | Out of Sample Error |
|----------|-----------------|---------------------|
| Accuracy |      0.9987     |        0.9961       |
| Kappa    |      0.9985     |        0.9951       |

As you can see, the accuracy estimate is slightly lower for the out of sample error estimate which makes sense as this data was not included in developing the model.  As a result, the out of sample accuracy and kappa rates are better estimates of the true out of sample error associated with this data as we have avoided over fitting our model as much as possible.
```{r}
#Check decision tree w/boosting results on my test dataset
table(predict(modFitBO, newdata=testing_new),testing_new$classe)


#Calculate the out of sample error based on this test dataset
check <- predict(modFitBO, newdata=testing_new) == testing_new$classe
out_sample_accuracy <- table(check)["TRUE"]/length(check)
out_sample_accuracy


#Run a confusion matrix to get out of sample error
confusionMatrix(predict(modFitBO, newdata=testing_new), testing_new$classe)
```

Application of model parameters to the 20 observation test dataset
------------------------------------------------------------------
The resulting boosting decision tree model was applied to the 20 data points provided without the "classe" values present.  This model was able to predict all 20 values accurately.
```{r}
#Read in the testing data set
#Let's see how great my model is in real life!
testing_all <- read.csv(file="pml-testing.csv",head=TRUE, sep=",")
new_predictions <- predict(modFitBO, newdata=testing_all)
new_predictions


#Save the predicted values to individual text files for upload
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(new_predictions)

```

